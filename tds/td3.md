# TD n¬∞3 - Conteneurisation ++

Les notions abord√©es durant ce TD seront li√©es √† `docker-compose` et √† la s√©curit√© des conteneurs.

## docker-compose

Nous allons utiliser `docker-compose` pour unifi√© le d√©ploiement d'une ensemble de conteneur sur votre machine.

Nous reprendrons les images ex√©cut√©es dans le projet de Virtualisation et Cloud Computing pour d√©ployer localement et en une commande l'architecture suivante :

```mermaid
graph LR
    F["Frontend
        API_URL='localhost:5000'"] --> B["API
                                        REDIS_URL='localhost'
                                        REDIS_PORT='6379'"]
    B --> R[(Redis DB)]
```
Rappel: 
- L'image du frontend devrait ressembler √† √ßa: `europe-west1-docker.pkg.dev/polytech-dijon/polytech-dijon/frontend-2024:nom1-nom2` (le conteneur exposera le port `8080`).
- l'image de l'API devrait ressembler √† √ßa: `europe-west1-docker.pkg.dev/polytech-dijon/polytech-dijon/backend-2024:nom1-nom2` (le conteneur exposera le port `5000`).
- Pour **Redis** utiliser: `redis` (le conteneur exposera le port `6379`).

> [!important]
> **Impossible de t√©l√©charger les images ?** R√©authentifiez vous avec la commande `gcloud auth login` et `gcloud auth configure-docker europe-west1-docker.pkg.dev`.
>
> **Toujours rien ?** Faites moi signe üëã

### Mise en place

Voici un template de fichier `compose.yaml` utilis√© par la commande `docker compose up` :

```yaml
services:
  frontend:
    image: <image>
    ports:
      - "<port-hote>:<port-ctn>"
    environment:
      - <key>=<value>
    network:
      - myapp-network

  backend:
    <to-do-for-backend>
  <other-services>
networks:
  # La pr√©sence de la ligne suivante suffit pour demander la cr√©ation du r√©seau.
  myapp-network: {}
```

√Ä partir du template ci-dessus et [de la documentation](https://docs.docker.com/compose/intro/compose-application-model/) :

1. Dans un dossier `td-conteneurisation`, cr√©er un fichier `compose.yaml`.
2. D√©finissez, pour chaque conteneur de l'architecture, un service dans le tableau `services`.
3. Validez le l'architecture en la d√©marrant localement avec la commande:

   ```shell
   docker compose up
   ```
   > Vous pouvez arr√™ter les diff√©rents conteneurs en une commande : `docker compose down`
4. Rendez-vous sur http://localhost:8080 üëà

> [!note]
> Le r√©seau virtuel `myapp-network`, dans lequel vont s'incrire ces conteneurs, n'a pas besoin param√®tre suppl√©mentaire.

### Du build au d√©ploiement

`docker-compose` permet √©galement d'unifi√© le d√©velopement des diff√©rents tiers de l'application.

1. Dans le dossier `td-conteneurisation`, cr√©er un dossier `frontend` et `backend`.
2. Ajouter les codes et les Dockerfile du `frontend` et de l'API du projet Virtualization Cloud Computing dans ces dossiers.
3. Pour chacun des services d√©finis dans le fichier `compose.yaml`, ajouter l'argument `build` qui permet de definir o√π se trouvent les fichiers sources des diff√©rents services de la composition.

    ```yaml
        # ...
        frontend:
          build:
            context: ./frontend/                      # Dossier d'ex√©cution du `docker build`
            dockerfile: frontend/Dockerfile           # Dockerfile √† utiliser pour l'ex√©cution du `docker build`
        # ...
    ```

4. Contruiser tous les conteneurs en une seule fois en utilisant la commande :

    ```shell
        docker compose build
    ```

## S√©curit√© des conteneurs

### Trivy

[Trivy](https://github.com/aquasecurity/trivy) est un outil de scan d'image qui permet d'identifier les vuln√©rabilit√©s d'une configuration.

En vous basant sur [la documentation de l'outil](https://trivy.dev/v0.57/getting-started/installation/), installer l'outil en suivant les √©tapes recommand√©es pour votre syst√®me.

Trivy s'utilise sur des configurations (Kubernetes, Terraform, etc) et des images en suivant le format de commande suivant:

```bash
trivy <target> [--scanners <scanner1,scanner2>] <subject>
```

Nous allons scanner notre premi√®re image, utiliser `trivy` pour scanner l'image `python:3.4-alpine`.

```bash
trivy image python:3.4-alpine
```

**Que pouvez-vous observer ?**

Nous allons maintenant scanner les images construites dans la parties pr√©c√©dentes.

1. Construiser les images de conteneur `frontend` et `backend` √† partir de leur Dockerfile et scanner ces images.
2. R√©solver les vuln√©rabilit√©s identifi√©es par `trivy`.
3. Reconstruiser et rescanner vos images, si des vuln√©rabilit√©s sont list√©es, reprennez **√† l'√©tape 2**.

**Un dernier test pour la route**

Scanner l'ensemble des fichiers du dossier de votre d√©p√¥t en local avec `trivy`.

> [!tip]
> Un ensemble de fichiers et de dossiers avec une racine commune est aussi appel√© "syst√®me de fichier" qui se traduit par "File System" en anglais et s'abr√®ge `fs`.

#### Bonus - Tout √ßa en plus simple

Trivy dispose d'une GitHub Action ! Via les informations de [ce d√©p√¥t](https://github.com/aquasecurity/trivy-action) et le Market Place des GitHub Action. Mettez en place sur votre d√©p√¥t une GitHub Action qui scan les images issues de vos Dockerfiles.

### Falco

[Falco](https://falco.org/) est un outil de s√©curit√© Cloud Natif qui assure la s√©curit√© √† l'ex√©cution des machines h√¥tes, des conteneurs, de Kubernetes, des environnements Cloud. C'est un outil actif qui surveille l'environnement dans lequel il est d√©ploy√© en temps r√©el.

Pour ce faire, Falco utilise le concept de **r√®gle**. Chaque r√®gle d√©crit un comportement √† risque et le niveau de criticit√© associ√© √† ce comportement. Si un de ces comportements est observ√©, Falco l'enregistre sous la forme d'√©venement.

Falco a √©t√© install√© au sein du cluster Kubernetes que nous avons utilis√© durant le module de **Virtualisation & Cloud Computing**.

Nous allons commencer par tester la configuration par d√©faut de Falco.

1. Connectez-vous au cluster.
2. Cr√©er un d√©ploiement pour une Pod ex√©cutant l'image `nginx`.

    ```shell
    kubectl create deployment nginx --image=nginx
    ```

3. Executons maintenant dans ce pod, un comportement suspect:

    ```shell
    kubectl exec -it $(kubectl get pods --selector=app=nginx -o name) -- cat /etc/shadow
    ```

    > [!info]
    > Cette commande tente d'afficher le contenu du fichier `shadow` stocker dans le dossier `/etc/` du conteneur ex√©cut√© (dossier administrateur).

4. Que pouvez-vous observ√© dans les logs de Falco ? (affichage des evenements de type `Warning`)

    ```shell
    kubectl logs -l app.kubernetes.io/name=falco -n falco -c falco | grep Warning
    ```

> [!tip]
> **F√©licitation !** Vous avez pu observer votre premier √©v√®nement de s√©curit√© avec [Falco](https://falco.org/).
>
> Curieux de savoir quelle √©tait cette r√®gle qui a √©t√© enfreinte ? Sa description est [ici](https://github.com/falcosecurity/rules/blob/c0a9bf17d5451340ab8a497efae1b8a8bd95adcb/rules/falco_rules.yaml#L398).


Vous allez maintenant cr√©er votre propre r√®gle et l'ajouter au catalogue de surveillance de Falco.

1. Cr√©er un fichier `my-falco-rule.yaml` √† partir du contenu suivant :

```yaml
customRules:
  custom-rules.yaml: |-
    - rule: Prevent exams
      desc: An attempt to create a exam.pdf file.
      condition: >
        (evt.type in (open,openat,openat2) and evt.is_open_write=true and fd.typechar='f' and fd.num>=0)
        and fd.name contains exam.pdf
      output: "File that look like an exam opened for writing (file=%fd.name pcmdline=%proc.pcmdline gparent=%proc.aname[2] ggparent=%proc.aname[3] gggparent=%proc.aname[4] evt_type=%evt.type user=%user.name user_uid=%user.uid user_loginuid=%user.loginuid process=%proc.name proc_exepath=%proc.exepath parent=%proc.pname command=%proc.cmdline terminal=%proc.tty %container.info)"
      priority: WARNING
      tags: [filesystem, mitre_persistence]    
```

2. Charger la r√®gle dans l'instance de Falco en ex√©cution dans le cluster.

    ```shell
    helm upgrade --namespace falco falco falcosecurity/falco --set tty=true -f falco_custom_rules_cm.yaml
    ```

    > [!note]
    > `helm` est un outil pour faciliter la manipulation de manifest `yaml`. [Installer HELM](https://helm.sh/fr/docs/intro/install/)

3. Attendez que l'instance de Falco est red√©marr√© avec la nouvelle r√®gle.

    ```shell
    kubectl wait pods --for=condition=Ready --all -n falco
    ```

4. Tester votre r√®gle.

    ```shell
    kubectl exec -it $(kubectl get ns -o name) -- touch /etc/exam.pdf
    ```

    ```shell
    kubectl logs -l app.kubernetes.io/name=falco -n falco -c falco | grep Warning
    ```

> [!tip]
> **Vous avez cr√©er votre premi√®re r√®gle Falco üöÄ**

## Bonus - Acc√©der √† l'interface pour lire les √©v√®nements

Falco poss√®de un projet annexe nomm√© **Falcosidekick**, offrant une interface utilisateur pour la lecture des √©v√®nements. **Falcosidekick** et **Falcosidekick UI** ont √©t√© ajout√© au cluster.

1. Utiliser le `port-forward` pour cr√©er un tunnel entre votre machine et le service **Falcosidekick UI**.

```shell
kubectl get -n falco service
```
```shell
kubectl -n falco port-forward svc/falco-falcosidekick-ui 2802
```

2. Acc√©der √† l'interface via votre navigateur pr√©f√©r√© en vous rendant sur http://localhost:2802/
